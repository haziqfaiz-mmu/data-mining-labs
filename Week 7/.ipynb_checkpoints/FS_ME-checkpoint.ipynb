{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mmu_logo.png\" style=\"height: 80px;\" align=left> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "Towards the end of this lesson, you should be able to:\n",
    "- experiments with 2 different feature selection algorithms\n",
    "- define different ways to evaluate a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Ranking Feature\n",
    "\n",
    "This is a function to do the ranking of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking(ranks, names, order=1):\n",
    "    minmax = MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x,2), ranks)\n",
    "    return dict(zip(names, ranks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"banking.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate into X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.y\n",
    "X = df.drop(\"y\", 1)\n",
    "colnames = X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Boruta classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the BorutaPy function\n",
    "# your codes here...\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Boruta classifier to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the ranking of the features returned by Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boruta_score = ranking(list(map(float, feat_selector.ranking_)), colnames, order=-1)\n",
    "boruta_score = pd.DataFrame(list(boruta_score.items()), columns=['Features', 'Score'])\n",
    "boruta_score = boruta_score.sort_values(\"Score\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top and Bottom 10 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------Top 10----------')\n",
    "display(boruta_score.head(10))\n",
    "\n",
    "print('---------Bottom 10----------')\n",
    "boruta_score.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_boruta_plot = sns.catplot(x=\"Score\", y=\"Features\", data = boruta_score[:], kind = \"bar\", \n",
    "               height=14, aspect=1.9, palette='coolwarm')\n",
    "plt.title(\"Boruta all Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare RFE classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Task:\n",
    "# Try the following params:\n",
    "# 1. class_weight={0:1,1:2}\n",
    "# 2. class_weight=\"balanced\"\n",
    "# 3. max_depth=4\n",
    "# 4. max_depth=6\n",
    "# 5. n_estimators = 100\n",
    "# observe the feature importance ranking.\n",
    "\n",
    "# your codes here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit RFE classifier to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the ranking of the features returned by RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_score = ranking(list(map(float, rfe.ranking_)), colnames, order=-1)\n",
    "rfe_score = pd.DataFrame(list(rfe_score.items()), columns=['Features', 'Score'])\n",
    "rfe_score = rfe_score.sort_values(\"Score\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_rfe_plot = sns.catplot(x=\"Score\", y=\"Features\", data = rfe_score[-10:], kind = \"bar\", \n",
    "               height=14, aspect=1.9, palette='coolwarm')\n",
    "plt.title(\"RFE bottom-10 Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model list using only NB and DT\n",
    "\n",
    "model_list = [\"NB\", \"DT\"]\n",
    "feature_num, acc_nb, acc_dt = [], [], []\n",
    "\n",
    "for i in range(1, 30):\n",
    "    feature_num.append(i)\n",
    "    for model in model_list:\n",
    "        \n",
    "        # Create X and y dataset\n",
    "        y = df.y\n",
    "        X = df.drop(\"y\", axis = 1)\n",
    "        \n",
    "        cols = boruta_score.Features[0:i]\n",
    "        X = X[cols].copy()\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "        \n",
    "        if model == \"NB\":\n",
    "            clf = GaussianNB()\n",
    "        elif model == \"DT\":\n",
    "            clf = DecisionTreeClassifier(max_depth=3) \n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        acc = round((accuracy_score(y_test, y_pred)*100), 2)\n",
    "        \n",
    "        if model == \"NB\":\n",
    "            acc_nb.append(acc)\n",
    "        elif model == \"DT\":\n",
    "            acc_dt.append(acc) \n",
    "            \n",
    "# your codes here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the line charts\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "ax = sns.lineplot(x = \"No_Of_Features\", y = \"Accuracy\", hue = \"Model\", data = boruta_acc_result)\n",
    "ax.set(ylim=(0, 100))\n",
    "ax.set(title=\"Accuracy Trend for Different Classifiers (Boruta)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model accuracy using different number of features\n",
    "\n",
    "# your codes here...\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "\n",
    "df = pd.read_csv('banking.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "\n",
    "df_X = df.drop('y', axis=1)\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummification of variables\n",
    "\n",
    "# your codes here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the train and test dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Decision Tree Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct Decision Tree Model\n",
    "\n",
    "# your codes here...\n",
    "\n",
    "y_pred = model_DT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy\n",
    "# your codes here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix report\n",
    "\n",
    "confusion_majority=confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('Mjority classifier Confusion Matrix\\n', confusion_majority)\n",
    "\n",
    "print('**********************')\n",
    "print('Mjority TN= ', confusion_majority[0][0])\n",
    "print('Mjority FP=', confusion_majority[0][1])\n",
    "print('Mjority FN= ', confusion_majority[1][0])\n",
    "print('Mjority TP= ', confusion_majority[1][1])\n",
    "print('**********************')\n",
    "\n",
    "print('Precision= {:.2f}'.format(precision_score(y_test, y_pred)))\n",
    "print('Recall= {:.2f}'. format(recall_score(y_test, y_pred)))\n",
    "print('F1= {:.2f}'. format(f1_score(y_test, y_pred)))\n",
    "print('Accuracy= {:.2f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC\n",
    "\n",
    "# your codes here...\n",
    "\n",
    "print('AUC: %.2f' % auc_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve \n",
    "\n",
    "# your codes here...\n",
    "\n",
    "plt.plot(fpr_DT, tpr_DT, color='blue', label='DT') \n",
    "plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall Curve\n",
    "from sklearn import metrics\n",
    "\n",
    "# your codes here...\n",
    "\n",
    "plt.plot(prec_DT, rec_DT, color='blue', label='DT') \n",
    "plt.plot([1, 0], [0.1, 0.1], color='green', linestyle='--')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    " \n",
    "# calculate precision-recall AUC \n",
    "# your codes here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(nb.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(nb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_majority=confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('Mjority classifier Confusion Matrix\\n', confusion_majority)\n",
    "\n",
    "print('**********************')\n",
    "print('Mjority TN= ', confusion_majority[0][0])\n",
    "print('Mjority FP=', confusion_majority[0][1])\n",
    "print('Mjority FN= ', confusion_majority[1][0])\n",
    "print('Mjority TP= ', confusion_majority[1][1])\n",
    "print('**********************')\n",
    "\n",
    "print('Precision= {:.2f}'.format(precision_score(y_test, y_pred)))\n",
    "print('Recall= {:.2f}'. format(recall_score(y_test, y_pred)))\n",
    "print('F1= {:.2f}'. format(f1_score(y_test, y_pred)))\n",
    "print('Accuracy= {:.2f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC\n",
    "\n",
    "# your codes here...\n",
    "\n",
    "print('AUC: %.2f' % auc_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve \n",
    "\n",
    "fpr_NB, tpr_NB, thresholds_NB = roc_curve(y_test, prob_NB) \n",
    "\n",
    "plt.plot(fpr_NB, tpr_NB, color='orange', label='NB') \n",
    "plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_NB, rec_NB, threshold_NB = precision_recall_curve(y_test, prob_NB)\n",
    "\n",
    "plt.plot(prec_NB, rec_NB, color='orange', label='NB') \n",
    "plt.plot([1, 0], [0.1, 0.1], color='green', linestyle='--')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "# calculate precision-recall AUC \n",
    "print(metrics.auc(rec_NB, prec_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr_NB, tpr_NB, color='orange', label='NB') \n",
    "plt.plot(fpr_DT, tpr_DT, color='blue', label='DT')  \n",
    "plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(prec_DT, rec_DT, color='blue', label='DT') \n",
    "plt.plot(prec_NB, rec_NB, color='orange', label='NB') \n",
    "\n",
    "plt.plot([1, 0], [0.1, 0.1], color='black', linestyle='--')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
